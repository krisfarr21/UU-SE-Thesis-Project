{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from camel_tools.utils.charmap import CharMapper # to transliterate Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def tag_freq(df: pd.DataFrame):\n",
    "    print(\"Number of tags: {}\".format(len(df.Tag.unique())))\n",
    "    frequencies = df.Tag.value_counts()\n",
    "    return frequencies\n",
    "\n",
    "def remove_extra_labels(old_new_labels: dict, df: pd.DataFrame):\n",
    "    df = df.replace(old_new_labels)\n",
    "    return df\n",
    "\n",
    "def change_dtypes(df: pd.DataFrame):\n",
    "    data_types_dict = {'Sentence #': str,\n",
    "                       'Word': str,\n",
    "                       'Tag': str}\n",
    "    df = df.astype(data_types_dict)\n",
    "    return df\n",
    "\n",
    "def create_sentences_word_labels_cols(df: pd.DataFrame):\n",
    "    df = df.fillna(method='ffill')\n",
    "    # let's create a new column called \"sentence\" which groups the words by sentence \n",
    "    df['sentence'] = df[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n",
    "    # let's also create a new column called \"word_labels\" which groups the tags by sentence \n",
    "    df['word_labels'] = df[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n",
    "    # keeping only the sentence and word_labels columns\n",
    "    df = df[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def num_of_sentences(dfs_dict: dict):\n",
    "    for df in dfs_dict.keys():\n",
    "        assert type(df) == pd.DataFrame\n",
    "    languages = [key for key in dfs_dict.keys()]\n",
    "    num_of_sentences = [int(df['Sentence #'].dropna().tolist()[-1][9:]) for df in dfs_dict.values()]\n",
    "    plt.bar(languages, num_of_sentences)\n",
    "    plt.grid(color='black', linestyle='--', linewidth=1, axis='y', alpha=1)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets paths\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "raw_datasets = dir_path + '/raw_datasets/'\n",
    "final_datasets = dir_path + '/final_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data formatted in .txt files\n",
      "['/domus/h1/krisfarr/thesis/raw_datasets/CamelLab_test.txt', '/domus/h1/krisfarr/thesis/raw_datasets/CamelLab_train.txt', '/domus/h1/krisfarr/thesis/raw_datasets/WikiFANE_Gold_2014_500K.txt']\n",
      "\n",
      " Data already formatted in .csv files\n",
      "['/domus/h1/krisfarr/thesis/raw_datasets/_dutch_data.csv', '/domus/h1/krisfarr/thesis/raw_datasets/_english_data.csv', '/domus/h1/krisfarr/thesis/raw_datasets/_italian_data.csv', '/domus/h1/krisfarr/thesis/raw_datasets/_maltese_test_data.csv', '/domus/h1/krisfarr/thesis/raw_datasets/_maltese_train_data.csv', '/domus/h1/krisfarr/thesis/raw_datasets/_spanish_data.csv']\n"
     ]
    }
   ],
   "source": [
    "# saving files into variables (according to their format)\n",
    "\n",
    "lang_dataframes = dict()\n",
    "data_txt = glob.glob(f\"{raw_datasets}*.txt\", recursive=True) # containing Arabic data\n",
    "data_csv = glob.glob(f\"{raw_datasets}*.csv\", recursive=True) # containing Maltese, Italian, Spanish, English and Dutch data (need less preprocessing)\n",
    "print(\"Data formatted in .txt files\", data_txt, \"\\n Data already formatted in .csv files\", data_csv, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic (fine-grained ANERCorp - CAMEL Lab + coarse-grained WikiFane)  Data\n",
    "\n",
    "https://camel.abudhabi.nyu.edu/anercorp/ + https://fsalotaibi.kau.edu.sa/Pages-Arabic-NE-Corpora.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tagged words from CamelLab_test.txt -> 25933\n",
      "END OF FILE--> /domus/h1/krisfarr/thesis/raw_datasets/CamelLab_test.txt\n",
      "number of tagged words from CamelLab_train.txt -> 129075\n",
      "END OF FILE--> /domus/h1/krisfarr/thesis/raw_datasets/CamelLab_train.txt\n",
      "number of tagged words from WikiFANE_Gold_2014_500K.txt -> 505324\n",
      "END OF FILE--> /domus/h1/krisfarr/thesis/raw_datasets/WikiFANE_Gold_2014_500K.txt\n"
     ]
    }
   ],
   "source": [
    "# Arabic text files into csv files (appending dataframes together for training)\n",
    "arabic_dfs = []\n",
    "sentence_number = 1\n",
    "\n",
    "for file_txt in data_txt:\n",
    "    file_name = re.findall('(?<=raw_datasets/).*$', file_txt)[0]\n",
    "    with open(file_txt) as f:\n",
    "        # changing tag 'PERS' to 'PER'; removing 'right to left' encoding;\n",
    "        lines = [line.replace('PERS', 'PER').replace('\\u200f', '') for line in f.readlines()]\n",
    "        lines = [line.replace('\\ufeff', '').replace('\\t', ' ').replace('\\u200e', '') for line in lines]\n",
    "        if file_name == 'WikiFANE_Gold_2014_500K.txt':\n",
    "            lines = [line.replace(' ', '') if line == ' \\n' else line.strip() for line in lines]\n",
    "        # initalizing sentence number and list for dataframe\n",
    "        df = []\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            if line == '\\n':\n",
    "                # if line is empty (sentence segmentation), append dummy word and tag and increment sentence number\n",
    "                df.append([f'Sentence: {sentence_number}', '0', 'O'])\n",
    "                sentence_number += 1\n",
    "                continue\n",
    "            # remove skip lines and split into list\n",
    "            line = line.strip().split(' ')\n",
    "\n",
    "            # add sentence number to first position\n",
    "            line.insert(0, f'Sentence: {sentence_number}')\n",
    "            # append line (type:lst) to dataframe\n",
    "            df.append(line)\n",
    "        # initialize column names\n",
    "        column_names = ['Sentence #', 'Word', 'Tag']\n",
    "        # change list to dataframe\n",
    "        df = pd.DataFrame(df, columns=column_names, dtype=str)\n",
    "        print(f\"number of tagged words from {file_name} -> {df.shape[0]}\")\n",
    "        # append dataframe to list of dataframes\n",
    "        arabic_dfs.append(df)\n",
    "        print(\"END OF FILE-->\", file_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660332, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate list of dataframes together\n",
    "arabic_data = pd.concat(arabic_dfs)\n",
    "arabic_data = change_dtypes(arabic_data)\n",
    "\n",
    "# number of rows, i.e. number of words and tags\n",
    "arabic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>505321</th>\n",
       "      <td>Sentence: 20661</td>\n",
       "      <td>م</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505322</th>\n",
       "      <td>Sentence: 20661</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505323</th>\n",
       "      <td>Sentence: 20661</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Sentence # Word Tag\n",
       "505321  Sentence: 20661    م   O\n",
       "505322  Sentence: 20661    .   O\n",
       "505323  Sentence: 20661    0   O"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags to change - manually chosen\n",
    "\n",
    "# tags starting with B\n",
    "to_change_B = {'B-Airport': 'B-LOC', 'B-Artist': 'B-PER', \n",
    "            'B-Athlete': 'B-PER', 'B-Building-Grounds': 'B-LOC',\n",
    "            'B-Businessperson': 'B-PER', 'B-Government': 'B-ORG', 'B-Continent': 'B-LOC',\n",
    "            'B-Group': 'B-ORG', 'B-Land-Region-Natural': 'B-LOC',\n",
    "            'B-Lawyer': 'B-PER', 'B-Nation': 'B-LOC',\n",
    "            'B-Non-Governmental': 'B-ORG',\n",
    "            'B-Other_PER': 'B-PER',\n",
    "            'B-Police': 'B-PER',\n",
    "            'B-Politician': 'B-PER',\n",
    "            'B-Population-Center': 'B-LOC',\n",
    "            'B-Religious_ORG': 'B-ORG',\n",
    "            'B-Religious_PER': 'B-PER',\n",
    "            'B-Scientist': 'B-PER',\n",
    "            'B-State-or-Province': 'B-LOC',\n",
    "            'B-Water-Body' : 'B-LOC'}\n",
    "\n",
    "# tags starting with I\n",
    "to_change_I = {'I-'+k[2:]:'I-'+v[2:] for k,v in to_change_B.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing chosen tags to PER, LOC and ORG\n",
    "\n",
    "arabic_data['Tag'] = arabic_data['Tag'].replace(to_change_B)\n",
    "arabic_data['Tag'] = arabic_data['Tag'].replace(to_change_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = {'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substituting any remaining tags with MISC\n",
    "\n",
    "arabic_data['Tag'] = arabic_data['Tag'].apply(lambda x: 'B-MISC' if x not in final_labels and x[0] == 'B' else x)\n",
    "arabic_data['Tag'] = arabic_data['Tag'].apply(lambda x: 'I-MISC' if x not in final_labels and x[0] == 'I' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that tags have been removed\n",
    "\n",
    "unique_tags = set(arabic_data['Tag'].tolist())\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_data.to_csv(f'testing_arabic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dataframes['arabic'] = arabic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maltese Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/domus/h1/krisfarr/thesis/raw_datasets/_maltese_test_data.csv',\n",
       " '/domus/h1/krisfarr/thesis/raw_datasets/_maltese_train_data.csv']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maltese_csv = [file for file in data_csv if 'maltese' in file]\n",
    "maltese_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    Sentence #      Word   POS Tag\n",
       " 0  Sentence: 1  kuntenta   ADJ   O\n",
       " 1          NaN        li  COMP   O,\n",
       " (5894, 4),\n",
       " (11151, 4))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking maltese data\n",
    "pd.read_csv(maltese_csv[0]).head(2), pd.read_csv(maltese_csv[0]).shape, pd.read_csv(maltese_csv[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating Maltese data\n",
    "\n",
    "maltese_dfs = []\n",
    "for data in maltese_csv:\n",
    "    with open(data) as f:\n",
    "        df = pd.read_csv(f, usecols=['Sentence #', 'Word', 'Tag'])\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        maltese_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17045, 3)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maltese_data = pd.concat(maltese_dfs, ignore_index=True)\n",
    "maltese_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  599\n"
     ]
    }
   ],
   "source": [
    "sentence_number = 1\n",
    "for i, row in maltese_data.iterrows():\n",
    "    if pd.notnull(row['Sentence #']):\n",
    "        row['Sentence #'] = f\"Sentence: {sentence_number}\"\n",
    "        sentence_number += 1\n",
    "\n",
    "print(f\"Number of sentences: {maltese_data['Sentence #'].dropna().tolist()[-1][9:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dataframes['maltese'] = maltese_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian, English, Spanish and Dutch data (already partially prepocessed from R&D project) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/domus/h1/krisfarr/thesis/raw_datasets/_dutch_data.csv',\n",
       " '/domus/h1/krisfarr/thesis/raw_datasets/_english_data.csv',\n",
       " '/domus/h1/krisfarr/thesis/raw_datasets/_italian_data.csv',\n",
       " '/domus/h1/krisfarr/thesis/raw_datasets/_spanish_data.csv']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_langs_csv = [file for file in data_csv if 'maltese' not in file]\n",
    "other_langs_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file in other_langs_csv:\n",
    "    file_name = re.findall('(?<=raw_datasets/).*$', csv_file)[0]\n",
    "    lang = re.search(r'_(.*?)_', file_name).group(1)\n",
    "    with open(csv_file) as f:\n",
    "        df = pd.read_csv(f)\n",
    "        if lang == 'english':\n",
    "            # English CoNLL 2003 dataset was fine-grained and therefore, extra tags need to be removed\n",
    "            # change to 'O' tags or 'MISC' tags?\n",
    "            df['Tag'] = df['Tag'].replace({'B-TIM': 'B-MISC', 'I-TIM': 'I-MISC',\n",
    "                        'B-GPE': 'B-MISC', 'I-GPE': 'I-MISC',\n",
    "                        'B-ART': 'B-MISC', 'I-ART': 'I-MISC',\n",
    "                        'B-EVE': 'B-MISC', 'I-EVE': 'I-MISC',\n",
    "                        'B-NAT': 'B-MISC', 'I-NAT': 'I-MISC'})\n",
    "        lang_dataframes[lang] = df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To add periods in English csv + slicing Italian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting full stops in English dataset\n",
    "english_data = lang_dataframes['english']\n",
    "cols = english_data.columns\n",
    "columns_lists = {}\n",
    "for col in cols:\n",
    "    columns_lists[col] = english_data[col].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_next = None\n",
    "list_sents, list_words = columns_lists['Sentence #'], columns_lists['Word']\n",
    "list_pos, list_ner = columns_lists['POS'], columns_lists['Tag']\n",
    "zipped_rows = list(zip(list_sents, list_words, list_pos, list_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_rows_ = []\n",
    "for i, row in enumerate(zipped_rows):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    if row == (np.nan, '0', '0', 'O'):\n",
    "        zipped_rows_.append((np.nan, '.', np.nan, 'O'))\n",
    "    zipped_rows_.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zipped_rows_, columns=['Sentence #', 'Word', 'POS', 'Tag'])\n",
    "df.to_csv('testing_english_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/krisfarr/thesis/raw_datasets/_italian_data.csv\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "    to_slice_index = df['Sentence #'].tolist().index('Sentence: 20001')\n",
    "    df_ = df.iloc[:583127, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.to_csv(\"/home/krisfarr/thesis/raw_datasets/_italian_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From CSVs to fully preprocessed for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC', 'B-ORG', 'I-PER', 'I-LOC', 'B-MISC', 'B-PER', 'O', 'I-ORG', 'I-MISC'}\n",
      " \n",
      "{'B-LOC': 0, 'B-ORG': 1, 'I-PER': 2, 'I-LOC': 3, 'B-MISC': 4, 'B-PER': 5, 'O': 6, 'I-ORG': 7, 'I-MISC': 8}\n",
      " \n",
      "{0: 'B-LOC', 1: 'B-ORG', 2: 'I-PER', 3: 'I-LOC', 4: 'B-MISC', 5: 'B-PER', 6: 'O', 7: 'I-ORG', 8: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "def unique_tags(df):\n",
    "    '''\n",
    "    Output: set of unique tags\n",
    "    '''\n",
    "    # language chosen arbitrarly\n",
    "    unique_tags = set(df['Tag'].tolist())\n",
    "    return unique_tags\n",
    "\n",
    "def tag2id(unique_tags):\n",
    "    '''\n",
    "    Output: dict of tags mapped to indices\n",
    "    '''\n",
    "    tag2id_ = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "    return tag2id_\n",
    "\n",
    "def id2tag(tag2id_):\n",
    "    '''\n",
    "    Output: dict of indices mapped to tags\n",
    "    '''\n",
    "    id2tag_ = {id: tag for tag, id in tag2id_.items()}\n",
    "    return id2tag_\n",
    "\n",
    "UNIQUE_TAGS = unique_tags(lang_dataframes[random.choice(list(lang_dataframes.keys()))])\n",
    "TAG2ID = tag2id(UNIQUE_TAGS)\n",
    "ID2TAG = id2tag(TAG2ID)\n",
    "FINAL_DATA = dict()\n",
    "\n",
    "print(UNIQUE_TAGS, TAG2ID, ID2TAG, sep='\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset():\n",
    "    def __init__(self, dataframe: pd.DataFrame, language: str):\n",
    "        self._df = dataframe\n",
    "        self._num_of_sents = self._df.shape[0]\n",
    "        self._lang = language\n",
    "\n",
    "        train_sentences = None\n",
    "        train_labels = None\n",
    "    \n",
    "    def get_df(self) -> pd.DataFrame:\n",
    "        return self._df\n",
    "    \n",
    "    def get_df_head(self):\n",
    "        return self._df.head()\n",
    "    \n",
    "    def get_df_tail(self):\n",
    "        return self._df.tail()\n",
    "    \n",
    "    def get_num_of_sentences(self):\n",
    "        return self._num_of_sents\n",
    "\n",
    "    def check_for_nulls(self):\n",
    "        return self._df.isnull().sum()\n",
    "\n",
    "    def assert_lengths(self):\n",
    "        print(\"asserting lengths...\")\n",
    "        print(f\"Lengths->  sentences: {len(self.train_sentences)} labels: {len(self.train_labels)}\")\n",
    "        assert len(self.train_sentences) == len(self.train_labels)\n",
    "\n",
    "        for i, sent in enumerate(self.train_sentences):\n",
    "            assert len(sent) == len(self.train_labels[i])\n",
    "    \n",
    "\n",
    "    def create_columns(self):\n",
    "        '''\n",
    "        Explained in comments.\n",
    "        '''\n",
    "        # \"forward fill\" function to fill missing values based on the last upper non-nan value\n",
    "        self._df = self._df.fillna(method='ffill')\n",
    "        # creating a  new column called \"sentence\" which groups the words by sentence \n",
    "        self._df['tokens'] = self._df[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n",
    "        # creating a new column called \"word_labels\" which groups the tags by sentence \n",
    "        self._df['ner_tags'] = self._df[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n",
    "        # keeping only the sentence and word_labels columns\n",
    "        self._df = self._df[[\"Sentence #\", \"tokens\", \"ner_tags\"]].drop_duplicates().reset_index(drop=True)\n",
    "        self._df.columns = ['id', 'tokens', 'ner_tags']\n",
    "        self._df['id'] = self._df['id'].str.slice(start=9)\n",
    "        print(\"Created ner_tags and tokens columns with values as lists! \\n\")\n",
    "        print(f\"Number of sentences: {df_.get_df().shape[0]}\")\n",
    "        return self._df\n",
    "    \n",
    "    def values_to_lists(self, delimiter_n=1):\n",
    "        '''\n",
    "        Changing string values to lists in tokens and ner_tags columns\n",
    "        '''\n",
    "\n",
    "        self.train_sentences = [sentence.split(' ')[:-delimiter_n] for sentence in self._df.tokens.values]\n",
    "        self.train_labels = [label.split(',')[:-delimiter_n] for label in self._df.ner_tags.values]\n",
    "\n",
    "        print(self.train_sentences[0], self.train_labels[0])\n",
    "\n",
    "        self.assert_lengths()\n",
    "        \n",
    "        # assert len(self.train_sentences[0]) == len(self.train_labels[1])\n",
    "\n",
    "    def update_lists(self):\n",
    "        '''\n",
    "        Encode labels and remove sentences tagged with 'O' only.\n",
    "        Update sentences in accordance with the removed labels.\n",
    "        '''\n",
    "    \n",
    "        train_labels_, remove_sentences = list(), list()\n",
    "        for i, sent_label in enumerate(self.train_labels):\n",
    "            # print(self.train_sentences[i])\n",
    "            if len(sent_label) < 3:\n",
    "                remove_sentences.append(self.train_sentences[i])\n",
    "                continue \n",
    "            # temporary list\n",
    "            sent_label_ = []\n",
    "            for label in sent_label:\n",
    "                # encode label\n",
    "                sent_label_.append(TAG2ID[label])\n",
    "\n",
    "            # rule does not apply to Maltese data given the limited number of sentences\n",
    "            if self._lang == 'maltese' or set(sent_label_) != {TAG2ID['O']}:\n",
    "                train_labels_.append(sent_label_)\n",
    "\n",
    "            # append to remvoe sentences\n",
    "            elif set(sent_label_) == {TAG2ID['O']}:\n",
    "                remove_sentences.append(self.train_sentences[i])\n",
    "        self.train_labels = train_labels_\n",
    "\n",
    "        for sent in remove_sentences:\n",
    "            self.train_sentences.remove(sent)\n",
    "\n",
    "        self.assert_lengths()\n",
    "\n",
    "    def update_columns(self):\n",
    "        # if self._lang == 'arabic':\n",
    "        #     train_sentences_ = []\n",
    "        #     for i, sent in enumerate(self.train_sentences):\n",
    "        #         sent.insert(len(sent), '.')\n",
    "        #         train_sentences_.append(sent)\n",
    "        #         self.train_labels[i].insert(len(train_labels), tag2id['O'])\n",
    "        self.final_df = self.get_df()\n",
    "        temp_df = df.iloc[:len(self.train_labels), :].copy()\n",
    "        temp_df['tokens'] = self.train_sentences\n",
    "        temp_df['ner_tags'] = self.train_labels\n",
    "        self.final_df = temp_df\n",
    "        \n",
    "    \n",
    "    def save_csv(self):\n",
    "        # df = self.get_df()\n",
    "        print(self.final_df.head(100))\n",
    "        self.final_df.to_csv(f'_{self._lang}_testing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE: maltese \n",
      "\n",
      "Number of words and tags: 17045 \n",
      "\n",
      "Checking for nulls: \n",
      " Sentence #    16446\n",
      "Word              0\n",
      "Tag               0\n",
      "dtype: int64 \n",
      "\n",
      "Created ner_tags and tokens columns with values as lists! \n",
      "\n",
      "Number of sentences: 599\n",
      "############################ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First stage of preprocessing\n",
    "for lang, df in lang_dataframes.items():\n",
    "    if lang=='maltese':\n",
    "        print(f\"LANGUAGE: {lang} \\n\")\n",
    "        df_ = PrepareDataset(dataframe=df, language=lang)\n",
    "        print(f\"Number of words and tags: {df_.get_num_of_sentences()} \\n\")\n",
    "        print(f\"Checking for nulls: \\n {df_.check_for_nulls()} \\n\")\n",
    "        df_.create_columns()\n",
    "        print(\"############################ \\n\")\n",
    "        FINAL_DATA[lang] = df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language maltese\n",
      "['kuntenta', 'li', 'erġajt', 'fuq', 'l-', 'art', ',', 'imma', 'diġà', 'għandi', 'nostalġija', 'għal', 'dawk', 'il-', 'kwiekeb', 'sbieħ', '.'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "asserting lengths...\n",
      "Lengths->  sentences: 599 labels: 599\n",
      "asserting lengths...\n",
      "Lengths->  sentences: 589 labels: 589\n",
      "     Sentence #       Word  POS    Tag  \\\n",
      "0   Sentence: 0  Melbourne   NP  B-LOC   \n",
      "1           NaN          (  Fpa      O   \n",
      "2           NaN  Australia   NP  B-LOC   \n",
      "3           NaN          )  Fpt      O   \n",
      "4           NaN          ,   Fc      O   \n",
      "..          ...        ...  ...    ...   \n",
      "95          NaN         de   SP      O   \n",
      "96          NaN        que   CS      O   \n",
      "97          NaN        las   DA      O   \n",
      "98          NaN   personas   NC      O   \n",
      "99          NaN        que   PR      O   \n",
      "\n",
      "                                               tokens  \\\n",
      "0   [kuntenta, li, erġajt, fuq, l-, art, ,, imma, ...   \n",
      "1                             [grazzi, ,, estelle, .]   \n",
      "2   [fis-, snien, ħamsin, kien, jikteb, l-, iżjed,...   \n",
      "3   [it-, tliet, opri, kbar, tiegħu, vaganzi, tas-...   \n",
      "4   [f', novembru, li, għadda, huwa, daqq, għall-,...   \n",
      "..                                                ...   \n",
      "95  [Sabiex, tesegwixxi, dik, il-, politika, ,, da...   \n",
      "96  [., Wieħed, mis-, setturi, f, pajjiżna, li, jo...   \n",
      "97  [L-, Uffiċċju, tal-, President, ,, dak, tal-, ...   \n",
      "98  [Il-, ġustizzja, ma, ssirx, bid-, deċiżjoni, ,...   \n",
      "99  [Dubai, hu, Emirat, wieħed, minn, fost, seba',...   \n",
      "\n",
      "                                             ner_tags  \n",
      "0   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
      "1                                        [6, 6, 5, 6]  \n",
      "2   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
      "3   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 6, 6, ...  \n",
      "4   [6, 6, 6, 6, 6, 6, 6, 5, 2, 6, 6, 0, 6, 4, 8, ...  \n",
      "..                                                ...  \n",
      "95  [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
      "96  [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
      "97  [6, 4, 8, 8, 6, 6, 6, 4, 6, 6, 6, 6, 1, 6, 6, ...  \n",
      "98  [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
      "99         [0, 6, 0, 6, 6, 6, 6, 6, 6, 6, 0, 3, 3, 6]  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "for lang, df_ in FINAL_DATA.items():\n",
    "    print(f\"Language {lang}\")\n",
    "    df_.values_to_lists()\n",
    "    df_.update_lists()\n",
    "    df_.update_columns()\n",
    "    df_.save_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PrepareDataset at 0x2b4085620438>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_DATA['maltese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italian\n",
      "original length 26424\n",
      "training should be 21139\n",
      "validation should be 5284 \n",
      "\n",
      "arabic\n",
      "original length 4897\n",
      "training should be 3917\n",
      "validation should be 979 \n",
      "\n",
      "english\n",
      "original length 47959\n",
      "training should be 38367\n",
      "validation should be 9591 \n",
      "\n",
      "spanish\n",
      "original length 8323\n",
      "training should be 6658\n",
      "validation should be 1664 \n",
      "\n",
      "dutch\n",
      "original length 15806\n",
      "training should be 12644\n",
      "validation should be 3161 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in LANGUAGES:\n",
    "    df = pd.read_csv(f'/home/krisfarr/thesis/final_datasets/{lang}_data.csv')\n",
    "    print(lang, f\"original length {len(df)}\", f\"training should be {int(len(df) * 0.8)}\", f\"validation should be {int(len(df) * 0.2)} \\n\",  sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "618f8975643fe55f2b184e1e6f9e588ff6e4c6b4579449737df5c70855a45068"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
